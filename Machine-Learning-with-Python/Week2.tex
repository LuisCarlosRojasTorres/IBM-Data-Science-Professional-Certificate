\chapter{Week 2}
\section{Linear Regression}

	\subsection{Introduction to Regression}	
	
	Hello and welcome! In this video we'll be giving a brief introduction to regression. So let's get started. Look at this data set. It's related to co2 emissions from different cars. 
	It includes engine size, number of cylinders, fuel consumption, and co2 emission from various automobile models. 
	
	The question is: given this data set can we predict the co2 emission of a car using other fields such as engine size or cylinders? Let's assume we have some historical data from different cars and assume that a car such as in row 9 has not been manufactured yet, but we're interested in estimating its approximate co2 emission after production. Is it possible? 
	
	We can use regression methods to predict a continuous value such as co2 emission using some other variables. Indeed regression is t\textbf{he process of predicting a continuous value}. 
	In regression there are \textbf{two types of variables}: a \textbf{dependent variable} and \textbf{one or more independent variables}. 
	
	The dependent variable can be seen as the state, target, or final goal we study and try to predict. 
	And the independent variables, also known as explanatory variables, can be seen as the causes of those states. The independent variables are shown conventionally by X and the dependent variable is notated by Y.
	
	A regression model relates Y or the dependent variable to a function of X i.e. the independent variables. 
	
	The key point in the regression is that our dependent value should be continuous and cannot be a discrete value. 
	However, the independent variable, or variables, can be measured on either a categorical or continuous measurement scale. 
	
	So, what we want to do here is to use the historical data of some cars using one or more of their features and from that data make a model. We use regression to build such a regression estimation model; then the model is used to predict the expected co2 emission for a new or unknown car. 
	
	Basically there are two types of regression models \textbf{simple regression} and \textbf{multiple regression}. Simple regression is when \textbf{one independent variable is used} to estimate a dependent variable. It \textbf{can be either linear or non-linear}. 
	For example, predicting co2 emission using the variable of engine size. 
	Linearity of regression is based on the nature of relationship between independent and dependent variables. When \textbf{more than one independent variable is present} the process is called \textbf{multiple linear regression}. For example, predicting co2 emission using engine size and the number of cylinders in any given car. Again, depending on the relation between dependent and independent variables it can be either linear or non-linear regression. 
	
	Let's examine some sample applications of regression. 
	Essentially we use regression when we want to estimate a continuous value. For instance, one of the applications of regression analysis could be in the area of sales forecasting. You can try to predict a sales person's total yearly sales from independent variables such as age, education, and years of experience. 
	It can also be used in the field of psychology, for example, to determine individual satisfaction, based on demographic and psychological factors. We can use regression analysis to predict the price of a house in an area, based on its size number of bedrooms, and so on. We can even use it to predict employment income for independent variables such as hours of work, education, occupation, sex, age, years of experience, and so on. 
	
	Indeed, you can find many examples of the usefulness of regression analysis in these and many other fields or domains, such as finance, healthcare, retail, and more. We have many regression algorithms, each of them has its own importance and a specific condition to which their application is best suited. And while we've covered just a few of them in this course, it gives you enough base knowledge for you to explore different regression techniques. 
	
	\subsection{Simple Linear Regression}
	
	
	In this video, we'll be covering linear regression. You don't need to know any linear algebra to understand topics in linear regression. This high-level introduction will give you enough background information on linear regression to be able to use it effectively on your own problems. So let's get started.
	
	Let's take a look at this data set. It's related to the Co2 emission of different cars. It includes engine size, cylinders, fuel consumption and Co2 emissions for various car models. The question is, given this data set, can we predict the Co2 emission of a car using another field such as engine size? 
	
	Quite simply, yes. We \textbf{can use linear regression to predict a continuous value such as Co2 emission by using other variables}. Linear regression is the approximation of a linear model used to describe the relationship between two or more variables. In simple linear regression, there are two variables, a dependent variable and an independent variable.
	The \textbf{key point} in the linear regression is that our dependent value \textbf{should be continuous and cannot be a discrete value}. However, the independent variables can be measured on either a categorical or continuous measurement scale. 
	
	There are two types of linear regression models. They are simple regression and multiple regression. Simple linear regression is when one independent variable is used to estimate a dependent variable. For example, predicting Co2 emission using the engine size variable. When more than one independent variable is present the process is called multiple linear regression, for example, predicting Co2 emission using engine size and cylinders of cars. Our focus in this video is on simple linear regression. 
	
	Now let's see how linear regression works. Okay, so let's look at our data set again. To understand linear regression, we can plot our variables here. We show engine size as an independent variable and emission as the target value that we would like to predict. A scatter plot clearly shows the relation between variables where changes in one variable explain or possibly cause changes in the other variable. Also, it indicates that these variables are linearly related. With linear regression \textbf{you can fit a line through the data}. For instance, as the engine size increases, so do the emissions. With linear regression you can model the relationship of these variables. A good model can be used to predict what the approximate emission of each car is.
	
	How do we use this line for prediction now?
	
	Let us assume for a moment that the line is a good fit of the data. We can use it to predict the emission of an unknown car. For example, for a sample car with engine size $2.4$, you can find the emission is $214$.
	
	Now, let's talk about what the fitting line actually is.
	
	We're going to predict the target value y. In our case using the independent variable engine size represented by $x_{1}$. The fit line is shown traditionally as a polynomial. In a simple regression problem, a single $x$, the form of the model would be $\theta_{0} + \theta_{1}x_{1}$. In this equation, y hat ($\hat{y}$) is the dependent variable of the predicted value. And $x_{1}$ is the independent variable.
	
	Theta 0 ($\theta_{0}$) and theta 1 ($\theta_{1}$) are the parameters of the line that we must adjust. Theta 1 is known as the slope or gradient of the fitting line and theta 0 is known as the intercept.
	
	Theta 0 ($\theta_{0}$) and theta 1 ($\theta_{1}$)are also called the \textbf{coefficients of the linear equation}.
	
	You can interpret this equation as y hat being a function of x1, or y hat being dependent of x1. How would you draw a line through the points? And how do you determine which line fits best?
	
	Linear regression estimates the coefficients of the line. This means we must calculate theta 0 and theta 1 to find the best line to fit the data. This line would best estimate the emission of the unknown data points. 
	
	Let's see how we can find this line or, to be more precise, how we can adjust the parameters to make the line the best fit for the data. For a moment, let's assume we've already found the best fit line for our data. Now, let's go through all the points and check how well they align with this line. Best fit here means that if we have, for instance, a car with engine size $x1 = 5.4$ and actual $Co2 = 250$, its Co2 should be predicted very close to the actual value, which is $y = 250$ based on historical data. But if we use the fit line, or better to say using our polynomial with known parameters to predict the Co2 emission, it will return $\hat{y}= 340$. Now if you compare the actual value of the emission of the car with what we've predicted using our model, you will find out that we have a $90$ unit error. This means our prediction line is not accurate. This error is also called the \textbf{residual error}. So we can say the \textbf{error is the distance from the data point to the fitted regression line}.
	
	\begin{equation}
		MSE = \frac{1}{n} \sum_{i=1}^{n}(y_i - \hat{y}_i)^{2}
	\end{equation}
	The mean of all residual errors shows how poorly the line fits with the whole data set. Mathematically it can be shown by the equation \textbf{Mean Squared Error}, shown as \textbf{MSE}. Our objective is to \textbf{find a line where the mean of all these errors is minimized}. In other words, the mean error of the prediction using the fit line should be minimized. Let's reword it more technically. The \textbf{objective of linear regression}, is to \textbf{minimize this MSE equation and to minimize it, we should find the best parameters theta 0 and theta 1}. Now the question is \textbf{how to find theta 0 and theta 1 in such a way that it minimizes this error?}
	
	How can we find such a perfect line? Or said another way, how should we find the best parameters for our line? Should we move the line a lot randomly and calculate the MSE value every time and choose the minimum one? Not really. Actually, we have \textbf{two options here}. Option one, we can use a \textbf{mathematic approach}, or option two, we can use an \textbf{optimization approach}. Let's see how we could easily use a mathematic formula to find the theta 0 and
	
	As mentioned before, theta 0 and theta 1 in the simple linear regression are the coefficients of the fit line. We can use a simple equation to estimate these coefficients. That is, given that it's a simple linear regression with only two parameters, and knowing that theta 0 and theta 1 are the intercept and slope of the line, we can estimate them directly from our data. It requires that we calculate the mean of the independent and dependent or target columns from the data set. Notice that all of the data must be available to traverse and calculate the parameters. It can be shown that the intercept and slope can be calculated using these equations.
	
	We can start off by estimating the value for theta 1. This is how you can find the slope of a line based on the data. X bar is the average value for the engine size in our data set. Please consider that we have nine rows here, rows 0 to 8. First we calculate the average of x1 and of y, then we plug it into the slope equation to find theta 1.
	
	The xi and yi in the equation refer to the fact that we need to repeat these calculations across all values in our data set. And i refers to the ith value of x or y. Applying all values, we find theta 1 equals 39. It is our second parameter. It is used to calculate the first parameter which is the intercept of the line.
	
	Now we can plug theta 1 into the line equation to find theta 0. It is easily calculated hat theta 0 equals 125.74. So these are the two parameters for the line, where theta 0 is also called the bias coefficient, and theta 1 is the coefficient for the Co2 emission column.
	
	As a side note, you really don't need to remember the formula for calculating these parameters, as \textbf{most of the libraries used for machine learning in Python}, R and Scala \textbf{can easily find these parameters for you}. But it's always good to understand how it works. Now, we can write down the polynomial of the line.
	
	\begin{equation}
		\hat{y} = \theta_{1}x +\theta_{0}
				= 39x + 125.74
	\end{equation}
	
	So we know how to find the best fit for our data and its equation. Now the question is how can we use it to predict the emission of a new car based on its engine size?
	
	After we found the parameters of the linear equation, making predictions is as simple as solving the equation for a specific set of inputs.
	
	Imagine we are predicting Co2 emission, or y, from engine size, or x for the automobile in record number 9. Our linear regression model representation for this problem would be $\hat{y} = \theta_{1}x +\theta_{0}$. Or if we map it to our data set, it would be $Co2Emission =\theta_{0} + \theta_{1} EngineSize$.
	
	As we saw, we can find theta 0, theta 1 using the equations that we just talked about. Once found, we can plug in the equation of the linear model. For example, let's use $\theta_{0} = 125$ and $\theta_{1} = 39$. So we can rewrite the linear model as $Co2Emission = 125 + 39 EngineSize$. Now let's plug in the 9th row of our data set and calculate the Co2 emission for a car with an engine size of 2.4. So $Co2Emission = 125 + 39 x 2.4$. Therefore, we can predict that the Co2Emission for this specific car would be $218.6$. 
	
	Let's talk a bit about why linear regression is so useful. Quite simply, it is the most basic regression to use and understand. In fact, one reason why linear regression is so useful is that \textbf{it's fast}. It also \textbf{doesn't require tuning of parameters}. So something like tuning the K parameter and K nearest neighbors, or the learning rate in neural networks isn't something to worry about. Linear regression is also \textbf{easy to understand}, and \textbf{highly interpretable}.
	
	\subsection{Model Evaluation in Regression Models}
	
	
	
	\subsection{Evaluation Metrics in Regression Models}
	
	\subsection{Multiple Linear Regression}

\section{Non-Linear Regression}

	\subsection{Non-Linear Regression}